{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../train.csv' # TODO: download train data and supply path here \n",
    "y, tX_starting, ids = load_csv_data(DATA_TRAIN_PATH,sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General data description\n",
    "pd_data=pd.read_csv(DATA_TRAIN_PATH)\n",
    "pd_data=pd_data.replace({'s':1,'b':-1})\n",
    "del pd_data['Id']\n",
    "pd_data.Prediction.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of nan\n",
    "pd_data[pd_data==-999].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "pd_data2=pd_data.replace({-999:0})\n",
    "corr_matrix2=pd_data2.corr()\n",
    "corr_matrix2.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_data3=pd_data.replace({-999:-10})\n",
    "corr_matrix3=pd_data3.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix=pd_data.corr()\n",
    "corr_matrix.Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synthesis=corr_matrix[['Prediction']].copy()\n",
    "synthesis['PredictionCorrected']=corr_matrix2.Prediction\n",
    "synthesis['PredictionNan-10']=corr_matrix3.Prediction\n",
    "synthesis['id']=range(-1,synthesis.shape[0]-1)\n",
    "synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(pd_data2.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(pd_data2[[24]],pd_data2[[22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names=pd_data.columns.values\n",
    "\n",
    "for i in range(tX_starting.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.title(names[i+1]+\" \"+str(i))\n",
    "    plt.hist(tX_starting[:,i],bins=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names=pd_data.columns.values\n",
    "\n",
    "for i in range(tX_starting.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.title(names[i+1]+\" \"+str(i))\n",
    "    plt.boxplot(tX_starting[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete columns with low correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_columns=[]\n",
    "for i in range(tX_starting.shape[1]):\n",
    "    coeff=np.corrcoef(y,tX_starting[:,i])[0,1]\n",
    "    if abs(coeff)<0.000:\n",
    "        drop_columns.append(i)\n",
    "tX=np.delete(tX_starting,drop_columns,axis=1)\n",
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_variable=22\n",
    "values=[0,1,2]\n",
    "\n",
    "added_matrix=np.zeros([tX.shape[0],3])\n",
    "added_matrix[:,0]=np.array([tX[:,22]==0])\n",
    "added_matrix[:,1]=np.array([tX[:,22]==1])\n",
    "added_matrix[:,2]=np.array([tX[:,22]==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX=np.delete(tX,[22],axis=1)\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(tX,degree,ones=True,columns_to_consider=False):\n",
    "    if not columns_to_consider:\n",
    "        columns_to_consider=range(tX.shape[1])\n",
    "    # Add ones column\n",
    "    if ones:\n",
    "        ones=np.ones(tX.shape[0]).reshape([tX.shape[0],1])\n",
    "        tX=np.concatenate((tX,ones),axis=1)\n",
    "    # Add power of the matrix\n",
    "    for i in range(2,degree+1):\n",
    "        tX=np.concatenate((tX,tX[:,columns_to_consider]**i),axis=1)\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 181)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX=tX_starting.copy()\n",
    "tX=build_poly(tX,6)\n",
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX=np.concatenate((tX,added_matrix),axis=1)\n",
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX[tX==-999]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalizing\n",
    "#mean=np.sum(tX,axis=0)/tX.shape[0]\n",
    "#std=np.sqrt(np.sum(tX**2,axis=0)/tX.shape[0])\n",
    "#tX=(tX-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    print(x.shape)\n",
    "    train_elements=int(ratio*x.shape[0])\n",
    "    test_elements=x.shape[0]-train_elements\n",
    "    print(train_elements,test_elements)\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    training_idx, test_idx = indices[:train_elements], indices[train_elements:]\n",
    "    x_train, x_test = x[training_idx], x[test_idx]\n",
    "    y_train, y_test = y[training_idx], y[test_idx]\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 181)\n",
      "125000 125000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(125000, 181)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train,tX_test,y_train,y_test=split_data(tX,y,0.5,1)\n",
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_predictions(tX,w):\n",
    "    prediction=tX.dot(w)\n",
    "    prediction[np.where(prediction <= 0)] = -1\n",
    "    prediction[np.where(prediction > 0)] = 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(prediction,y):\n",
    "    return (sum(y*prediction)/y.shape[0]+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y,tX,w):\n",
    "    prediction=compute_predictions(tX,w)\n",
    "    return evaluate_prediction(prediction,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tX, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    error= y-tX.dot(w)\n",
    "    square=np.sum(error**2)/error.shape[0]\n",
    "    return square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tX, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N=tX.shape[0]\n",
    "    error=y-tX.dot(w)\n",
    "    gradient=-1.0/N*(np.transpose(tX).dot(error))\n",
    "    return gradient\n",
    "        \n",
    "compute_gradient(y_train,tX_train,np.zeros([tX_train.shape[1]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tX, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # Compute gradient and loss\n",
    "        gradient=compute_gradient(y,tX,w)\n",
    "        loss=compute_loss(y,tX,w)\n",
    "        # Update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    print(w.shape)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "    N=tx.shape[0]\n",
    "    error=y-tx.dot(w)\n",
    "    gradient=-1.0/N*(np.transpose(tx).dot(error))\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_epochs, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    y_shuffle=[]\n",
    "    tx_shuffle=[]\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "        y_shuffle.append(minibatch_y)\n",
    "        tx_shuffle.append(minibatch_tx)\n",
    "    for n_iter in range(max_epochs):\n",
    "        # compute stochastic gradient\n",
    "        gradient=compute_stoch_gradient(y_shuffle[n_iter],tx_shuffle[n_iter],w)\n",
    "        loss=compute_loss(y,tx,w)\n",
    "        # update w\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.38470000e+02,   5.16550000e+01,   9.78270000e+01, ...,\n",
       "          3.63521508e+00,   2.29853552e+02,   2.13750084e+12],\n",
       "       [  1.60937000e+02,   6.87680000e+01,   1.03235000e+02, ...,\n",
       "          9.94014980e+17,   9.94014980e+17,   9.75703560e+09],\n",
       "       [ -9.99000000e+02,   1.62172000e+02,   1.25953000e+02, ...,\n",
       "          9.94014980e+17,   9.94014980e+17,   7.50824675e+09],\n",
       "       ..., \n",
       "       [  1.05457000e+02,   6.05260000e+01,   7.58390000e+01, ...,\n",
       "          9.94014980e+17,   9.94014980e+17,   5.48276155e+09],\n",
       "       [  9.49510000e+01,   1.93620000e+01,   6.88120000e+01, ...,\n",
       "          9.94014980e+17,   9.94014980e+17,   0.00000000e+00],\n",
       "       [ -9.99000000e+02,   7.27560000e+01,   7.08310000e+01, ...,\n",
       "          9.94014980e+17,   9.94014980e+17,   0.00000000e+00]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from implementations import *\n",
    "\n",
    "gradient_descent = least_squares_GD\n",
    "\n",
    "tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time=2.207 seconds\n",
      "[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan]\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: RuntimeWarning: invalid value encountered in less_equal\n",
      "  app.launch_new_instance()\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "w, mse = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "#gradient_losses, w = stochastic_gradient_descent(y_train, tX_train, w_initial,30, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "print(w)\n",
    "print(mse)\n",
    "print(evaluate(y_train, tX_train, w))\n",
    "print(evaluate(y_test, tX_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (125000,) (125000,181) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-cfc45229b29c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleast_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-cb229bd50919>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(y, tX, w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-dd2886fbd2cb>\u001b[0m in \u001b[0;36mevaluate_prediction\u001b[0;34m(prediction, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (125000,) (125000,181) "
     ]
    }
   ],
   "source": [
    "mse,w=least_squares(y_train,tX_train)\n",
    "print(evaluate(y_train,tX_train,w))\n",
    "print(evaluate(y_test,tX_test,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    w=np.linalg.solve(tx.T.dot(tx)+lamb**2*np.identity(tx.shape[1]),tx.T.dot(y))\n",
    "    mse=sum((y-tx.dot(w))**2)/tx.shape[0]\n",
    "    return mse,w\n",
    "\n",
    "mse,w=ridge_regression(y_train,tX_train,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perc_tr=[]\n",
    "perc_te=[]\n",
    "\n",
    "lambdas = np.linspace(-2,2,101)\n",
    "for lamb in lambdas:\n",
    "    mse,w=ridge_regression(y_train,tX_train,lamb)\n",
    "    \n",
    "    perc_tr.append(evaluate(y_train,tX_train,w))\n",
    "    perc_te.append(evaluate(y_test,tX_test,w))\n",
    "    \n",
    "\n",
    "plt.plot(lambdas,perc_tr,label='train',color='r')\n",
    "plt.plot(lambdas,perc_te,label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic implementation of logistic regression using the least squares\n",
    "def logistic_regression(y,tx,tx_test,threshold=0.5):\n",
    "    mse,w=least_squares(y,tx)\n",
    "    \n",
    "    y_prev=tx.dot(w)\n",
    "    y_prev=1/(1+np.exp(-y_prev))\n",
    "    output_train=np.ones(y_prev.shape[0])\n",
    "    output_train[np.where(y_prev<threshold)] = -1\n",
    "    \n",
    "    y_test=tx_test.dot(w)\n",
    "    y_test=1/(1+np.exp(-y_test))\n",
    "    output_test=np.ones(y_test.shape[0])\n",
    "    output_test[np.where(y_test<threshold)] = -1\n",
    "    \n",
    "    return output_train,output_test\n",
    "\n",
    "thresholds=np.linspace(0.48,0.52,101)\n",
    "perc_log_tr=[]\n",
    "perc_log_te=[]\n",
    "for threshold in thresholds:\n",
    "    output_train,output_test=logistic_regression(y_train,tX_train,tX_test,threshold)\n",
    "    perc_log_tr.append(evaluate_prediction(output_train,y_train))\n",
    "    perc_log_te.append(evaluate_prediction(output_test,y_test))\n",
    "\n",
    "    \n",
    "plt.plot(thresholds,perc_log_tr,'r',label='Train')\n",
    "plt.plot(thresholds,perc_log_te,'b',label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test_post=np.copy(tX_test)\n",
    "tX_test_post[tX_test_post==-999]=0\n",
    "prediction=compute_predictions(tX_test,w)\n",
    "evaluate_prediction(prediction,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sum((y_test-tX_test.dot(w))**2)/tX_test.shape[0])\n",
    "print(tX_test.dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_final_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_final_test=build_poly(tX_final_test,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_final_test)\n",
    "y_train,y_test = logistic_regression(y_train,tX_train,tX_final_test,0.48)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH,sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX[tX==-999]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "   \n",
    "    train_elements=int(ratio*x.shape[0])\n",
    "    test_elements=x.shape[0]-train_elements\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    training_idx, test_idx = indices[:train_elements], indices[train_elements:]\n",
    "    x_train, x_test = x[training_idx], x[test_idx]\n",
    "    y_train, y_test = y[training_idx], y[test_idx]\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX,tX_test,y,y_test=split_data(tX,y,0.5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tX, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    error= y-tX.dot(w)\n",
    "    square=np.sum(error**2)/error.shape[0]\n",
    "    return square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_gradient(y, tX, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N=tX.shape[0]\n",
    "    error=y-tX.dot(w)\n",
    "    gradient=-1.0/N*(np.transpose(tX).dot(error))\n",
    "    return gradient\n",
    "        \n",
    "compute_gradient(y,tX,np.zeros([tX.shape[1]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tX, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # Compute gradient and loss\n",
    "        gradient=compute_gradient(y,tX,w)\n",
    "        loss=compute_loss(y,tX,w)\n",
    "        # Update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    print(w.shape)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500,)\n",
      "Gradient Descent(0/49): loss=1.1621731318975859\n",
      "(2500,)\n",
      "Gradient Descent(1/49): loss=1.0210069446586643\n",
      "(2500,)\n",
      "Gradient Descent(2/49): loss=0.9555107808008292\n",
      "(2500,)\n",
      "Gradient Descent(3/49): loss=0.9184598813501934\n",
      "(2500,)\n",
      "Gradient Descent(4/49): loss=0.8939820746612764\n",
      "(2500,)\n",
      "Gradient Descent(5/49): loss=0.8761998360704168\n",
      "(2500,)\n",
      "Gradient Descent(6/49): loss=0.8625857747860897\n",
      "(2500,)\n",
      "Gradient Descent(7/49): loss=0.8518430546182695\n",
      "(2500,)\n",
      "Gradient Descent(8/49): loss=0.8431950718743124\n",
      "(2500,)\n",
      "Gradient Descent(9/49): loss=0.8361254148925437\n",
      "(2500,)\n",
      "Gradient Descent(10/49): loss=0.8302689958937933\n",
      "(2500,)\n",
      "Gradient Descent(11/49): loss=0.8253585483439821\n",
      "(2500,)\n",
      "Gradient Descent(12/49): loss=0.8211942116442952\n",
      "(2500,)\n",
      "Gradient Descent(13/49): loss=0.8176243395426603\n",
      "(2500,)\n",
      "Gradient Descent(14/49): loss=0.8145325810351578\n",
      "(2500,)\n",
      "Gradient Descent(15/49): loss=0.8118288388892766\n",
      "(2500,)\n",
      "Gradient Descent(16/49): loss=0.8094427829528907\n",
      "(2500,)\n",
      "Gradient Descent(17/49): loss=0.8073191138629744\n",
      "(2500,)\n",
      "Gradient Descent(18/49): loss=0.8054140571945014\n",
      "(2500,)\n",
      "Gradient Descent(19/49): loss=0.8036927389063708\n",
      "(2500,)\n",
      "Gradient Descent(20/49): loss=0.8021272015984643\n",
      "(2500,)\n",
      "Gradient Descent(21/49): loss=0.800694892734812\n",
      "(2500,)\n",
      "Gradient Descent(22/49): loss=0.7993775043874053\n",
      "(2500,)\n",
      "Gradient Descent(23/49): loss=0.7981600773661017\n",
      "(2500,)\n",
      "Gradient Descent(24/49): loss=0.7970303058945485\n",
      "(2500,)\n",
      "Gradient Descent(25/49): loss=0.7959779955243845\n",
      "(2500,)\n",
      "Gradient Descent(26/49): loss=0.7949946388706001\n",
      "(2500,)\n",
      "Gradient Descent(27/49): loss=0.7940730824170629\n",
      "(2500,)\n",
      "Gradient Descent(28/49): loss=0.7932072640291384\n",
      "(2500,)\n",
      "Gradient Descent(29/49): loss=0.792392005568861\n",
      "(2500,)\n",
      "Gradient Descent(30/49): loss=0.791622848591091\n",
      "(2500,)\n",
      "Gradient Descent(31/49): loss=0.7908959238097076\n",
      "(2500,)\n",
      "Gradient Descent(32/49): loss=0.7902078471030438\n",
      "(2500,)\n",
      "Gradient Descent(33/49): loss=0.7895556364149511\n",
      "(2500,)\n",
      "Gradient Descent(34/49): loss=0.7889366451428703\n",
      "(2500,)\n",
      "Gradient Descent(35/49): loss=0.7883485085566417\n",
      "(2500,)\n",
      "Gradient Descent(36/49): loss=0.7877891005360925\n",
      "(2500,)\n",
      "Gradient Descent(37/49): loss=0.7872564984919403\n",
      "(2500,)\n",
      "Gradient Descent(38/49): loss=0.7867489547924473\n",
      "(2500,)\n",
      "Gradient Descent(39/49): loss=0.7862648733693409\n",
      "(2500,)\n",
      "Gradient Descent(40/49): loss=0.7858027904582914\n",
      "(2500,)\n",
      "Gradient Descent(41/49): loss=0.7853613586462955\n",
      "(2500,)\n",
      "Gradient Descent(42/49): loss=0.7849393335743539\n",
      "(2500,)\n",
      "Gradient Descent(43/49): loss=0.7845355627746773\n",
      "(2500,)\n",
      "Gradient Descent(44/49): loss=0.7841489762332041\n",
      "(2500,)\n",
      "Gradient Descent(45/49): loss=0.7837785783510572\n",
      "(2500,)\n",
      "Gradient Descent(46/49): loss=0.7834234410438512\n",
      "(2500,)\n",
      "Gradient Descent(47/49): loss=0.7830826977748412\n",
      "(2500,)\n",
      "Gradient Descent(48/49): loss=0.7827555383525526\n",
      "(2500,)\n",
      "Gradient Descent(49/49): loss=0.7824412043653243\n",
      "(30,)\n",
      "Gradient Descent: execution time=0.006 seconds\n",
      "[ 1.44543849  0.86432458  1.02441176 ...,  1.26364189  0.85379217\n",
      "  0.89377899]\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([  4.75758725e-05,  -7.63351052e-03,  -5.69522020e-03,  -1.49293390e-03,\n",
    "   1.27062143e-02,   1.99314297e-04,  -3.63164395e-02,   2.96059220e-01,\n",
    "   1.15862514e-03,  -3.54969018e+01,  -1.83435975e-01,   1.10762995e-01,\n",
    "   4.41439627e-02,   3.55071732e+01,   9.46339832e-03,  -8.26812836e-04,\n",
    "   3.55102482e+01,   1.92830657e-02,   8.61388242e-03,   4.14729806e-03,\n",
    "  -1.87055446e-03,  -6.94875104e-04,  -3.51632971e-01,   5.29297059e-04,\n",
    "  -1.15698925e-02,   1.14756782e-02,  -7.02426871e-04,  -4.68553776e-03,\n",
    "  -1.47919941e-02,   3.54963346e+01])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, w = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "y_test2=np.reshape(y_test,[y_test.shape[0],1])\n",
    "print(sum((y_test2-tX_test.dot(w))**2)/tX_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672576523066 [  4.30590865e-04  -8.27598691e-03  -3.42198322e-03  -1.43340069e-04\n",
      "   5.35789809e-02  -2.04295329e-05  -1.19540052e-02   7.65451420e-02\n",
      "   5.27455054e-05  -1.05573316e+01  -2.85087577e-01   1.08888192e-01\n",
      "   4.43366853e-01   1.05607366e+01   2.40887161e-03  -3.07212668e-03\n",
      "   1.05693183e+01   1.61033091e-02  -6.41410528e-03   1.36421722e-03\n",
      "  -4.28180869e-03  -8.70391095e-04  -3.08246414e-02   1.79979901e-03\n",
      "  -3.53920334e-02  -1.05755003e-02   1.65768811e-03   1.40691173e-02\n",
      "  -1.04138883e-02   1.05554442e+01]\n"
     ]
    }
   ],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    # returns mse, and optimal weights\n",
    "    \n",
    "    w=np.linalg.solve(tx.T.dot(tx),tx.T.dot(y))\n",
    "    mse=sum((y-tx.dot(w))**2)/tx.shape[0]\n",
    "    \n",
    "    return mse,w\n",
    "    \n",
    "mse,w=least_squares(y,tX)\n",
    "print(mse,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672612941033 [  4.27723923e-04  -8.28676856e-03  -3.41350363e-03  -1.45155167e-04\n",
      "   5.47098722e-02  -2.30182953e-05  -1.21117742e-02   7.62748596e-02\n",
      "   5.51716961e-05   1.28486936e-03  -2.84015768e-01   1.09024144e-01\n",
      "   4.37193955e-01   2.13358017e-03   2.58018574e-03  -3.08595434e-03\n",
      "   1.06729840e-02   1.59267293e-02  -6.55886628e-03   1.36342347e-03\n",
      "  -4.25037376e-03  -8.69292690e-04  -3.04499089e-02   1.80274097e-03\n",
      "  -3.53329563e-02  -1.06811438e-02   1.66964184e-03   1.40043476e-02\n",
      "  -1.02886726e-02  -3.17934565e-03]\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    w=np.linalg.solve(tx.T.dot(tx)+lamb**2*np.identity(tx.shape[1]),tx.T.dot(y))\n",
    "    mse=sum((y-tx.dot(w))**2)/tx.shape[0]\n",
    "    return mse,w\n",
    "\n",
    "mse,w=ridge_regression(y,tX,1)\n",
    "print(mse,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679605906039\n",
      "[ 0.27628744 -0.44686622 -0.06140565 ..., -1.0844022  -0.67594269\n",
      " -0.59443859]\n"
     ]
    }
   ],
   "source": [
    "print(sum((y_test-tX_test.dot(w))**2)/tX_test.shape[0])\n",
    "print(tX_test.dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ..., -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../../predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

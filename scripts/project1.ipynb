{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH,sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX[tX==-999]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "   \n",
    "    train_elements=int(ratio*x.shape[0])\n",
    "    test_elements=x.shape[0]-train_elements\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    training_idx, test_idx = indices[:train_elements], indices[train_elements:]\n",
    "    x_train, x_test = x[training_idx], x[test_idx]\n",
    "    y_train, y_test = y[training_idx], y[test_idx]\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX,tX_test,y,y_test=split_data(tX,y,0.5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tX, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    error= y-tX.dot(w)\n",
    "    square=np.sum(error**2)/error.shape[0]\n",
    "    return square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_gradient(y, tX, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N=tX.shape[0]\n",
    "    error=y-tX.dot(w)\n",
    "    gradient=-1.0/N*(np.transpose(tX).dot(error))\n",
    "    return gradient\n",
    "        \n",
    "compute_gradient(y,tX,np.zeros([tX.shape[1]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tX, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # Compute gradient and loss\n",
    "        gradient=compute_gradient(y,tX,w)\n",
    "        loss=compute_loss(y,tX,w)\n",
    "        # Update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    print(w.shape)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "    N=tx.shape[0]\n",
    "    error=y-tx.dot(w)\n",
    "    gradient=-1.0/N*(np.transpose(tx).dot(error))\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_epochs, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    y_shuffle=[]\n",
    "    tx_shuffle=[]\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "        y_shuffle.append(minibatch_y)\n",
    "        tx_shuffle.append(minibatch_tx)\n",
    "    for n_iter in range(max_epochs):\n",
    "        # compute stochastic gradient\n",
    "        gradient=compute_stoch_gradient(y_shuffle[n_iter],tx_shuffle[n_iter],w)\n",
    "        loss=compute_loss(y,tx,w)\n",
    "        # update w\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=1.1621731318975859, w0=-0.0006080896299745168, w1=-0.008086000054730322\n",
      "Gradient Descent(1/49): loss=1.182549387096353, w0=-0.0004019002028903217, w1=-0.007947804726932399\n",
      "Gradient Descent(2/49): loss=1.0408976563021413, w0=-0.0009539435121557994, w1=-0.008326259154036498\n",
      "Gradient Descent(3/49): loss=1.0702035266298557, w0=-0.0005295827638817042, w1=-0.008258249968588371\n",
      "Gradient Descent(4/49): loss=1.3637854357794938, w0=-0.0011204292017032889, w1=-0.008526560791519727\n",
      "Gradient Descent(5/49): loss=0.9149360872331422, w0=-0.0009678842405886535, w1=-0.008688848663584927\n",
      "Gradient Descent(6/49): loss=0.8727613399051908, w0=-0.0008282857420919562, w1=-0.008767932961501966\n",
      "Gradient Descent(7/49): loss=1.000394526490803, w0=-0.0010208852971381015, w1=-0.008956766444394047\n",
      "Gradient Descent(8/49): loss=0.9912911388660175, w0=-0.001000993179751137, w1=-0.00913498151906656\n",
      "Gradient Descent(9/49): loss=0.8990605110130014, w0=-0.0005063241227658529, w1=-0.009073540637462038\n",
      "Gradient Descent(10/49): loss=0.9650176481972353, w0=-0.0010491873608835692, w1=-0.00942868405173212\n",
      "Gradient Descent(11/49): loss=0.9030249115845532, w0=-0.0008548395655554255, w1=-0.00926647711922992\n",
      "Gradient Descent(12/49): loss=0.999776019606861, w0=-0.0013670317623956912, w1=-0.009608253997741557\n",
      "Gradient Descent(13/49): loss=0.8339466501157272, w0=-0.000865767120461573, w1=-0.009606597529455987\n",
      "Gradient Descent(14/49): loss=1.514873503720114, w0=-0.0006255089083462421, w1=-0.009641860738716522\n",
      "Gradient Descent(15/49): loss=0.9510068540218887, w0=-0.0003946778842838383, w1=-0.009675226598644747\n",
      "Gradient Descent(16/49): loss=0.9898618062795799, w0=-0.0006232264264967097, w1=-0.009879418511436354\n",
      "Gradient Descent(17/49): loss=0.8938952361087853, w0=-0.0009102908472512084, w1=-0.010079799797900479\n",
      "Gradient Descent(18/49): loss=0.8312025742893792, w0=-0.0009033090158480207, w1=-0.010146194154769363\n",
      "Gradient Descent(19/49): loss=1.3010117832433579, w0=-0.0004995297351708162, w1=-0.010051083494188595\n",
      "Gradient Descent(20/49): loss=1.0970791001687135, w0=-0.0008347359801277772, w1=-0.010150656408051023\n",
      "Gradient Descent(21/49): loss=0.8558273934527267, w0=-0.001065063181107695, w1=-0.009992134549346408\n",
      "Gradient Descent(22/49): loss=0.8881127615659421, w0=-0.001385993625748843, w1=-0.010178950474390295\n",
      "Gradient Descent(23/49): loss=0.8155666546192399, w0=-0.001615423679386893, w1=-0.010309096421104546\n",
      "Gradient Descent(24/49): loss=0.9432971593369275, w0=-0.0011733078858848492, w1=-0.010256391953428359\n",
      "Gradient Descent(25/49): loss=0.9510356766264464, w0=-0.0013039194956371884, w1=-0.010337208734990645\n",
      "Gradient Descent(26/49): loss=0.8431053734965525, w0=-0.0014177124641259725, w1=-0.010434453184594279\n",
      "Gradient Descent(27/49): loss=0.8676600444814414, w0=-0.0013018810366803681, w1=-0.010354869833546262\n",
      "Gradient Descent(28/49): loss=1.0805801365934184, w0=-0.0016431082289937882, w1=-0.010411623939812025\n",
      "Gradient Descent(29/49): loss=1.2748107221369138, w0=-0.0009675197188074555, w1=-0.010221966100026398\n",
      "Gradient Descent(30/49): loss=1.2227166905070699, w0=-0.0016336741037108497, w1=-0.010410337701761386\n",
      "Gradient Descent(31/49): loss=0.9705990911380612, w0=-0.001369196667805519, w1=-0.010257644792520467\n",
      "Gradient Descent(32/49): loss=0.8907078441373604, w0=-0.0013463410514556846, w1=-0.010237291863779077\n",
      "Gradient Descent(33/49): loss=0.9509499056444969, w0=-0.0006731244438932276, w1=-0.010138736154226599\n",
      "Gradient Descent(34/49): loss=0.9494840961107657, w0=-0.0011539146370931866, w1=-0.010362986277008346\n",
      "Gradient Descent(35/49): loss=0.8132911642641005, w0=-0.0010293612841939393, w1=-0.010288348446314004\n",
      "Gradient Descent(36/49): loss=0.9087800453630646, w0=-0.0017995855054589358, w1=-0.010451273529189533\n",
      "Gradient Descent(37/49): loss=1.1173084700931972, w0=-0.0009974125394173776, w1=-0.010280508961097286\n",
      "Gradient Descent(38/49): loss=0.8905977434668206, w0=-0.0009304465604347546, w1=-0.01030210553323105\n",
      "Gradient Descent(39/49): loss=0.854952549993792, w0=-0.0009918261754447003, w1=-0.010416439326307949\n",
      "Gradient Descent(40/49): loss=0.8718218951728686, w0=-0.0009492172540370763, w1=-0.010512701240284576\n",
      "Gradient Descent(41/49): loss=0.9037844912214849, w0=-0.0009653179892102281, w1=-0.010523495667737102\n",
      "Gradient Descent(42/49): loss=0.8147574154735432, w0=-0.0009313139443306117, w1=-0.010590515013672171\n",
      "Gradient Descent(43/49): loss=0.7951942160385239, w0=-0.0006032396458189745, w1=-0.010549337618000296\n",
      "Gradient Descent(44/49): loss=1.03299833403477, w0=-0.001099748288988569, w1=-0.010721987125399632\n",
      "Gradient Descent(45/49): loss=0.8153790138830307, w0=-0.0011918374171349642, w1=-0.010724685969667705\n",
      "Gradient Descent(46/49): loss=0.833754333879001, w0=-0.0005000219707375401, w1=-0.010545283878367527\n",
      "Gradient Descent(47/49): loss=0.9288683219728279, w0=-0.00046511292354569565, w1=-0.010524855254945878\n",
      "Gradient Descent(48/49): loss=0.8313629338205967, w0=-0.00016774839502547837, w1=-0.010447931783854253\n",
      "Gradient Descent(49/49): loss=1.4530862794599801, w0=-0.0009626609136054719, w1=-0.010758268747703031\n",
      "Gradient Descent: execution time=0.007 seconds\n",
      "3.32766383856\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([  4.75758725e-05,  -7.63351052e-03,  -5.69522020e-03,  -1.49293390e-03,\n",
    "   1.27062143e-02,   1.99314297e-04,  -3.63164395e-02,   2.96059220e-01,\n",
    "   1.15862514e-03,  -3.54969018e+01,  -1.83435975e-01,   1.10762995e-01,\n",
    "   4.41439627e-02,   3.55071732e+01,   9.46339832e-03,  -8.26812836e-04,\n",
    "   3.55102482e+01,   1.92830657e-02,   8.61388242e-03,   4.14729806e-03,\n",
    "  -1.87055446e-03,  -6.94875104e-04,  -3.51632971e-01,   5.29297059e-04,\n",
    "  -1.15698925e-02,   1.14756782e-02,  -7.02426871e-04,  -4.68553776e-03,\n",
    "  -1.47919941e-02,   3.54963346e+01])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "#gradient_losses, w = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "gradient_losses, w = stochastic_gradient_descent(y, tX, w_initial,30, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "print(sum((y_test-tX_test.dot(w))**2)/tX_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672576523066 [  4.30590865e-04  -8.27598691e-03  -3.42198322e-03  -1.43340069e-04\n",
      "   5.35789809e-02  -2.04295329e-05  -1.19540052e-02   7.65451420e-02\n",
      "   5.27455054e-05  -1.05573316e+01  -2.85087577e-01   1.08888192e-01\n",
      "   4.43366853e-01   1.05607366e+01   2.40887161e-03  -3.07212668e-03\n",
      "   1.05693183e+01   1.61033091e-02  -6.41410528e-03   1.36421722e-03\n",
      "  -4.28180869e-03  -8.70391095e-04  -3.08246414e-02   1.79979901e-03\n",
      "  -3.53920334e-02  -1.05755003e-02   1.65768811e-03   1.40691173e-02\n",
      "  -1.04138883e-02   1.05554442e+01]\n"
     ]
    }
   ],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    # returns mse, and optimal weights\n",
    "    \n",
    "    w=np.linalg.solve(tx.T.dot(tx),tx.T.dot(y))\n",
    "    mse=sum((y-tx.dot(w))**2)/tx.shape[0]\n",
    "    \n",
    "    return mse,w\n",
    "    \n",
    "mse,w=least_squares(y,tX)\n",
    "print(mse,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672612941033 [  4.27723923e-04  -8.28676856e-03  -3.41350363e-03  -1.45155167e-04\n",
      "   5.47098722e-02  -2.30182953e-05  -1.21117742e-02   7.62748596e-02\n",
      "   5.51716961e-05   1.28486936e-03  -2.84015768e-01   1.09024144e-01\n",
      "   4.37193955e-01   2.13358017e-03   2.58018574e-03  -3.08595434e-03\n",
      "   1.06729840e-02   1.59267293e-02  -6.55886628e-03   1.36342347e-03\n",
      "  -4.25037376e-03  -8.69292690e-04  -3.04499089e-02   1.80274097e-03\n",
      "  -3.53329563e-02  -1.06811438e-02   1.66964184e-03   1.40043476e-02\n",
      "  -1.02886726e-02  -3.17934565e-03]\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    w=np.linalg.solve(tx.T.dot(tx)+lamb**2*np.identity(tx.shape[1]),tx.T.dot(y))\n",
    "    mse=sum((y-tx.dot(w))**2)/tx.shape[0]\n",
    "    return mse,w\n",
    "\n",
    "mse,w=ridge_regression(y,tX,1)\n",
    "print(mse,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980256005514\n",
      "[ 0.55725296 -0.29949886  0.14561944 ..., -0.89775117 -0.30239231\n",
      " -0.12083606]\n"
     ]
    }
   ],
   "source": [
    "print(sum((y_test-tX_test.dot(w))**2)/tX_test.shape[0])\n",
    "print(tX_test.dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ..., -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../../predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

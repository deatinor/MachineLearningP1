{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../train.csv' # TODO: download train data and supply path here \n",
    "y_starting, tX_starting, ids = load_csv_data(DATA_TRAIN_PATH,sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing that we made consists in different steps:\n",
    "    1. Delete the columns with low correlation with y (not used since it does not improve the results)\n",
    "    2. Replace the nan in the feature matrix with the median of the feature\n",
    "    3. Add 3 dummy variable corresponding to the 3 existing nan pattern. Fully removing the nan is not a valid solution since they seem correlated with y.\n",
    "    4. Replace the existing categorical variable with dummy variables.\n",
    "    5. Adding more features\n",
    "    6. Normalize the data \n",
    "    7. Add a column of all 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function before_poly() summarize the first 4 points.\n",
    "* The function build_poly() add all the wanted features\n",
    "* The function process_data() automatically call all this methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median, mean and standard deviation, computed in the train matrix are returned. \n",
    "They are used to normalize and to replace nan in the test matrix.\n",
    "We *don't* use the median, mean and standard deviation of the test matrix for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete columns with low correlation - replacing nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function was not used since it does not improve the results\n",
    "\n",
    "def drop_col(tX_starting):\n",
    "    drop_columns=[]\n",
    "    #for i in range(tX_starting.shape[1]):\n",
    "    #    coeff=np.corrcoef(y,tX_starting[:,i])[0,1]\n",
    "    #    if abs(coeff)<0.000:\n",
    "    #        drop_columns.append(i)\n",
    "\n",
    "\n",
    "    tX=np.delete(tX_starting,drop_columns,axis=1)\n",
    "    return tX\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_nan(tX_starting,median=False):\n",
    "    tX=tX_starting.copy()\n",
    "    # Dummy features added corresponding to the nan pattern\n",
    "    nan_position=[tX[:,[0,4,23]]!=-999][0]*1\n",
    "\n",
    "    for col in range(tX.shape[1]):\n",
    "        column=tX[:,col][tX[:,col]!=-999]\n",
    "        if median==False:\n",
    "            mean=column.mean()\n",
    "            median=np.median(column)\n",
    "\n",
    "        tX[:,col][tX[:,col]==-999]=median    \n",
    "    \n",
    "    return nan_position,tX,median\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categorical_variables(tX_starting):\n",
    "    tX=tX_starting.copy()\n",
    "    \n",
    "    cat_variable=22\n",
    "    values=[0,1,2]\n",
    "\n",
    "    added_matrix=np.zeros([tX.shape[0],3])\n",
    "    added_matrix[:,0]=np.array([tX[:,22]==0])\n",
    "    added_matrix[:,1]=np.array([tX[:,22]==1])\n",
    "    added_matrix[:,2]=np.array([tX[:,22]==2])\n",
    "    \n",
    "    tX=np.delete(tX,[22],axis=1)\n",
    "    \n",
    "    return added_matrix,tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function before_poly() calls automatically all the previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def before_poly(tX_starting,median=False):\n",
    "    tX=drop_col(tX_starting)\n",
    "    nan_position,tX,median=replace_nan(tX,median)\n",
    "    #added_matrix,tX=categorical_variables(tX)\n",
    "    #full_added_matrix=np.concatenate((added_matrix,nan_position),axis=1)\n",
    "    return nan_position,tX,median\n",
    "    return full_added_matrix,tX,median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple kind of features added:\n",
    "    1. Powers of the existing features (not categorical, not dummy variables) up to a specified degree.\n",
    "    2. Square root and half powers (i.e. 3/2,5/2,7/2) up to a specified degree\n",
    "    3. Exponential of the existing features (not categorical, not dummy variables)\n",
    "    4. Cross products of the features (i.e. x1*x2,x1*x3..). It is possible to retain only the more correlated ones with y, but this possibility was not used for the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_poly(tX,degree,y,prod_to_exclude=False,train=True,columns_to_consider=False,exponential=False,cross_products=False,added_matrix_for_cross=False,threshold_power=0.0,threshold_cross=0.00,exclude=False):\n",
    "    # Some feature can be not considered\n",
    "    if not columns_to_consider:\n",
    "        columns_to_consider=range(tX.shape[1])\n",
    "    # Cross products to exclude\n",
    "    if not prod_to_exclude:\n",
    "        prod_to_exclude=[]\n",
    "    if exclude==False:\n",
    "        exclude=[]\n",
    "    dict_cross={}\n",
    "    \n",
    "    # Features to include in hte model\n",
    "    columns_to_consider=[x for x in columns_to_consider if x not in exclude]\n",
    "    columns_to_consider=np.array(columns_to_consider)\n",
    "    # Add power of the matrix\n",
    "    final_list=[]\n",
    "    for i in range(2,degree+1):\n",
    "        cols=columns_to_consider\n",
    "        tX=np.concatenate((tX,tX[:,cols]**i),axis=1)\n",
    "    for i in range(2,18):\n",
    "        if i%2==1:\n",
    "            tX=np.concatenate((tX,np.sqrt(abs(tX[:,cols]**i))),axis=1)\n",
    "    # Take the exponential of the features\n",
    "    if exponential:\n",
    "        tX=np.concatenate((tX,np.exp(tX[:,cols]/100)),axis=1)\n",
    "        tX=np.concatenate((tX,np.exp(tX[:,cols]/80)),axis=1)\n",
    "        tX=np.concatenate((tX,np.exp(tX[:,cols]/60)),axis=1)\n",
    "        tX=np.concatenate((tX,np.exp(tX[:,cols]/50)),axis=1)\n",
    "        tX=np.concatenate((tX,np.exp(tX[:,cols]/40)),axis=1)\n",
    "        tX=np.concatenate((tX,np.exp(tX[:,cols]/20)),axis=1)\n",
    "\n",
    "    # Cross products of the features\n",
    "    if cross_products:\n",
    "        # The dummy variables are considered for the cross products\n",
    "        if added_matrix_for_cross.any():\n",
    "            # Add to columns to consider\n",
    "            for i in range(tX.shape[1],tX.shape[1]+added_matrix_for_cross.shape[1]):\n",
    "                columns_to_consider=np.append(columns_to_consider,i)\n",
    "            # Concatenate\n",
    "            tX=np.concatenate((tX,added_matrix_for_cross),axis=1)\n",
    "            final_list.append(tX)\n",
    "        start_cross=tX.shape[1]\n",
    "        for i,col1 in enumerate(columns_to_consider):\n",
    "            for j,col2 in enumerate(columns_to_consider):\n",
    "                if j>i and (i,j) not in prod_to_exclude:\n",
    "                    if train:\n",
    "                        prod=tX[:,col1]*tX[:,col2]\n",
    "                        corr=np.corrcoef(prod,y)[0,1]\n",
    "                        if abs(corr)>threshold_cross:\n",
    "                            final_list.append(prod.reshape([prod.shape[0],1]))\n",
    "\n",
    "                            #print(start_cross,type(start_cross))\n",
    "                            dict_cross[start_cross]=tuple([i,j])\n",
    "                            start_cross+=1\n",
    "                        else:\n",
    "                            prod_to_exclude.append((i,j))\n",
    "                    else:\n",
    "                        prod=tX[:,col1]*tX[:,col2]\n",
    "                        final_list.append(prod.reshape([prod.shape[0],1]))\n",
    "        final_tuple=tuple(final_list)\n",
    "        tX=np.concatenate(final_tuple,axis=1)\n",
    "    return tX,prod_to_exclude,dict_cross\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If train==False the mean and the std are the ones computed in the train matrix.\n",
    "def normalize(tX,mean=False,std=False,train=False):\n",
    "    if train:\n",
    "        mean=np.sum(tX,axis=0)/tX.shape[0]\n",
    "        std=np.sqrt(np.sum(tX**2,axis=0)/tX.shape[0])\n",
    "    tX=(tX-mean)/std\n",
    "    if train:\n",
    "        return tX,mean,std\n",
    "    else:\n",
    "        return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_ones(tX_starting):\n",
    "    ones=np.ones(tX_starting.shape[0]).reshape([tX_starting.shape[0],1])\n",
    "    tX=np.concatenate((tX_starting,ones),axis=1)\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to discriminate between the train and the test matrix.\n",
    "The median, mean and std computed in the train matrix are used in the test one.\n",
    "prod_to_exclude are the cross products to exclude in the test matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_data(tX_starting,y,prod_to_exclude=False,train=True,mean=False,std=False,median=False,exclude=False):\n",
    "    full_added_matrix,tX,median=before_poly(tX_starting,median)\n",
    "    tX,prod_to_exclude,dict_cross=build_poly(tX,14,y,exclude=exclude,train=train,prod_to_exclude=prod_to_exclude,exponential=True,cross_products=True,added_matrix_for_cross=full_added_matrix,threshold_cross=0.0)\n",
    "    if train:\n",
    "        tX,mean,std=normalize(tX,train=True)\n",
    "    else:\n",
    "        tX=normalize(tX,mean,std,train=False)\n",
    "    tX=add_ones(tX)\n",
    "    \n",
    "    if train:\n",
    "        return tX,prod_to_exclude,mean,std,median,dict_cross\n",
    "    else:\n",
    "        return tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CALL THIS FOR CROSS VALIDATION\n",
    "tX=tX_starting.copy()\n",
    "y=y_starting.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CALL THIS FOR COMPUTING PREDICTIONS WITHOUT CROSS VALIDATION (FOR SUBMISSION)\n",
    "exc=[]\n",
    "tX=tX_starting.copy()\n",
    "y=y_starting.copy()\n",
    "tX,prod_to_exclude,mean,std,median,dict_cross=process_data(tX,y,train=True,exclude=exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to create the indices to split train and test matrix in Cross Validation\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    #np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "k_indices=build_k_indices(y,4,1)\n",
    "np.save(\"k_indices\",k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that operates the cross validation with a specified k and lambda_ (for ridge regression)\n",
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    # ***************************************************\n",
    "    \n",
    "    loss_tr=[]\n",
    "    loss_te=[]\n",
    "    ac_tr=[]\n",
    "    ac_te=[]\n",
    "    w_vector=[]\n",
    "    for k_index in k_indices:\n",
    "        \n",
    "        x_test=x[k_index]\n",
    "        y_test=y[k_index]\n",
    "        \n",
    "        mask = np.ones(x.shape[0], dtype=bool) # all elements included/True.\n",
    "        mask[k_index] = False              # Set unwanted elements to False\n",
    "\n",
    "        x_train=x[mask]\n",
    "        y_train=y[mask]\n",
    "        exc=[]\n",
    "        x_train,prod_to_exclude,mean,std,median,dict_cross=process_data(x_train,y_train,train=True,exclude=exc)\n",
    "        x_test=process_data(x_test,y_test,prod_to_exclude=prod_to_exclude,mean=mean,std=std,median=median,train=False,exclude=exc)\n",
    "\n",
    "        w,mse=ridge_regression(y_train,x_train,lambda_)\n",
    "        w_vector.append(w)\n",
    "        ac_tr.append(evaluate(y_train,x_train,w))\n",
    "        ac_te.append(evaluate(y_test,x_test,w))\n",
    "        \n",
    "        \n",
    "    return ac_tr,ac_te,np.mean(ac_tr), np.mean(ac_te),w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamb=-5e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS TO TEST THE CROSS VALIDATION\n",
    "ac_tr,ac_te,mean_tr,mean_te,w=cross_validation(y,tX,k_indices,4,lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mean_te,mean_tr,ac_te,ac_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next script is used to optimize the lambda in the ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perc_tr=[]\n",
    "perc_te=[]\n",
    "\n",
    "lambdas = np.linspace(-0.00008,-0.00001,5)\n",
    "for i,lamb in enumerate(lambdas):\n",
    "    ac_tr,ac_te,mean_tr,mean_te,w=cross_validation(y,tX,k_indices,4,lamb)\n",
    "    perc_tr.append(mean_tr)\n",
    "    perc_te.append(mean_te)\n",
    "    print(mean_tr,mean_te,lamb)\n",
    "    \n",
    "    if i%1==0:\n",
    "        print(i)\n",
    "\n",
    "plt.plot(lambdas,perc_tr,label='train',color='r')\n",
    "plt.plot(lambdas,perc_te,label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the chosen lambda, compute the weights using the function ridge_regression().\n",
    "Then check the predictions in the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lamb=-6e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w,loss=ridge_regression(y,tX,lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(evaluate(y,tX,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../test.csv' \n",
    "_, tX_final_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tX_final=process_data(tX_final_test,y=y,prod_to_exclude=prod_to_exclude,mean=mean,std=std,median=median,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../predictions.csv' \n",
    "y_pred = predict_labels(w, tX_final)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
